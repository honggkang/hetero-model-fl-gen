from torch.utils.data import DataLoader, Dataset
from torch import nn
import torch
import torch.nn.functional as F
from torch.autograd import Variable
import numpy as np
from tqdm import tqdm
import copy


class DatasetSplit(Dataset):
    def __init__(self, dataset, idxs):
        self.dataset = dataset
        self.idxs = list(idxs)

    def __len__(self):
        return len(self.idxs)

    def __getitem__(self, item):
        image, label = self.dataset[self.idxs[item]]
        return image, label


class DatasetSplit_CelebA(Dataset):
    def __init__(self, dataset, idxs):
        self.dataset = dataset
        self.idxs = list(idxs)

    def __len__(self):
        return len(self.idxs)

    def __getitem__(self, item):
        image, attr = self.dataset[self.idxs[item]]
        return image, attr[20] # return gender attribute
    

##########################################
#                  GeFL                  #
##########################################

class LocalUpdate(object):
    def __init__(self, args, dataset=None, idxs=None):
        self.args = args
        self.selected_clients = []
        if args.dataset == 'celebA':
            self.ldr_train = DataLoader(DatasetSplit_CelebA(dataset, idxs), batch_size=args.local_bs, shuffle=True)        
        else:
            self.ldr_train = DataLoader(DatasetSplit(dataset, idxs), batch_size=args.local_bs, shuffle=True)

        if args.dataset == 'celebA':
            self.loss_func = nn.BCEWithLogitsLoss()  # For multi-label classification
        else:
            self.loss_func = nn.CrossEntropyLoss()  # For single-label classification

        if len(idxs)//args.local_bs == 0:
            self.iter = 1
            self.less_samples = True
            self.mini_bs = len(idxs)
        else:
            self.iter = len(idxs)//args.local_bs
            self.less_samples = False
        
    def train(self, net, learning_rate, gennet=None):
        net.train()

        optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=self.args.momentum, weight_decay=self.args.weight_decay)
        gen_epoch_loss = None
        gen_loss = None
        if gennet:
            gen_epoch_loss = []
            gennet.eval()
            # print('gen sample iteration',self.iter)
            for iter in range(self.args.local_ep_gen): # train by samples generated by generator
                gen_batch_loss = []
        
                for i in range(self.iter):
                    with torch.no_grad():
                        if self.less_samples:
                            images, labels = gennet.sample_image(self.args, sample_num=self.mini_bs) # images.shape (bs, feature^2)
                        else:
                            images, labels = gennet.sample_image(self.args, sample_num=self.args.local_bs) # images.shape (bs, feature^2)
                    net.zero_grad()
                    logits, log_probs = net(images)
                    loss = self.loss_func(logits, labels)
                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()

                    gen_batch_loss.append(loss.item())
                gen_epoch_loss.append(sum(gen_batch_loss)/len(gen_batch_loss))     
            # gennet creates feature samples (gennet(, labels))
            if gen_epoch_loss:
                gen_loss = sum(gen_epoch_loss) / len(gen_epoch_loss)

        # train and update
        optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=self.args.momentum, weight_decay=self.args.weight_decay)
        epoch_loss = []

        for iter in range(self.args.local_ep): # train net performing main-task
            batch_loss = []

            for batch_idx, (images, labels) in enumerate(self.ldr_train):
                images = images.to(self.args.device)
                labels = labels.unsqueeze(-1).float().to(self.args.device)
                net.zero_grad()
                logits, log_probs = net(images)
                loss = self.loss_func(logits, labels)
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                batch_loss.append(loss.item())
            epoch_loss.append(sum(batch_loss)/len(batch_loss))

        return net.state_dict(), sum(epoch_loss) / len(epoch_loss), gen_loss


# train only headers of Target networks #
class LocalUpdate_header(object):
    def __init__(self, args, dataset=None, idxs=None):
        self.args = args
        self.loss_func = nn.CrossEntropyLoss()
        self.selected_clients = []
        self.ldr_train = DataLoader(DatasetSplit(dataset, idxs), batch_size=args.local_bs, shuffle=True)
        if len(idxs)//args.local_bs == 0:
            self.iter = 1
            self.less_samples = True
            self.mini_bs = len(idxs)
        else:
            self.iter = len(idxs)//args.local_bs
            self.less_samples = False        
        
    def train(self, net, learning_rate, gennet=None, feature_extractor=None):
        net.train()
        feature_extractor.eval()

        optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=self.args.momentum, weight_decay=self.args.weight_decay)
        gen_epoch_loss = None
        gen_loss = None
        if gennet:
            gen_epoch_loss = []
            gennet.eval()
            # print('gen sample iteration',self.iter)
            for iter in range(self.args.local_ep_gen): # train by samples generated by generator
                gen_batch_loss = []
        
                for i in range(self.iter):
                    with torch.no_grad():
                        if self.less_samples:
                            images, labels = gennet.sample_image(self.args, sample_num=self.mini_bs) # images.shape (bs, feature^2)                        
                        else:
                            images, labels = gennet.sample_image(self.args, sample_num=self.args.local_bs) # images.shape (bs, feature^2)
                    net.zero_grad()
                    logits, log_probs = net(images, start_layer='feature')
                    loss = F.cross_entropy(logits, labels) # net.fc1.weight.grad / net.fc5.weight.grad
                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()

                    gen_batch_loss.append(loss.item())
                gen_epoch_loss.append(sum(gen_batch_loss)/len(gen_batch_loss))     
            # gennet creates feature samples (gennet(, labels))
            if gen_epoch_loss:
                gen_loss = sum(gen_epoch_loss) / len(gen_epoch_loss)

        # train and update
        optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=self.args.momentum, weight_decay=self.args.weight_decay)
        epoch_loss = []

        for iter in range(self.args.local_ep): # train net performing main-task
            batch_loss = []

            for batch_idx, (images, labels) in enumerate(self.ldr_train):
                images, labels = images.to(self.args.device), labels.to(self.args.device)
                images = feature_extractor(images)
                net.zero_grad()
                logits, log_probs = net(images, start_layer = 'feature')
                loss = F.cross_entropy(logits, labels)
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                batch_loss.append(loss.item())
            epoch_loss.append(sum(batch_loss)/len(batch_loss))
        if epoch_loss:
            avg_ep_loss = sum(epoch_loss) / len(epoch_loss)
        else:
            avg_ep_loss = -1
        
        return net.state_dict(), avg_ep_loss, gen_loss
    
    
#   Only synthetic data (NAS)   #
class LocalUpdate_onlyGen(object):
    def __init__(self, args, dataset=None, idxs=None):
        self.args = args
        self.loss_func = nn.CrossEntropyLoss()
        self.selected_clients = []
        self.ldr_train = DataLoader(DatasetSplit(dataset, idxs), batch_size=args.local_bs, shuffle=True)
        self.iter = len(idxs)//args.local_bs
        
    def train(self, net, learning_rate, gennet=None):
        net.train()

        optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=self.args.momentum, weight_decay=self.args.weight_decay)
        gen_epoch_loss = None
        gen_loss = None
        if gennet:
            gen_epoch_loss = []
            gennet.eval()
            # print('gen sample iteration',self.iter)
            for iter in range(self.args.local_ep): # Enough local epochs for synthetic data
                gen_batch_loss = []
        
                for i in range(self.iter):
                    with torch.no_grad():
                        images, labels = gennet.sample_image(self.args, sample_num=self.args.local_bs) # images.shape (bs, feature^2)
                    net.zero_grad()
                    logits, log_probs = net(images)
                    loss = F.cross_entropy(logits, labels) # net.fc1.weight.grad / net.fc5.weight.grad
                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()

                    gen_batch_loss.append(loss.item())
                gen_epoch_loss.append(sum(gen_batch_loss)/len(gen_batch_loss))     
            # gennet creates feature samples (gennet(, labels))
            if gen_epoch_loss:
                gen_loss = sum(gen_epoch_loss) / len(gen_epoch_loss)
            
        return net.state_dict(), -1, gen_loss
    
##########################################
#                  FedProx               #
##########################################

class LocalUpdate_fedprox(object):
    def __init__(self, args, dataset=None, idxs=None):
        self.args = args
        self.loss_func = nn.CrossEntropyLoss()
        self.selected_clients = []
        self.ldr_train = DataLoader(DatasetSplit(dataset, idxs), batch_size=args.local_bs, shuffle=True)
        
    def train(self, net, learning_rate):
        net.train()
        global_model = copy.deepcopy(net)
        # train and update
        optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=self.args.momentum, weight_decay=self.args.weight_decay)
        epoch_loss = []
        gen_loss = None

        for iter in range(self.args.local_ep): # train net performing main-task
            batch_loss = []

            for batch_idx, (images, labels) in enumerate(self.ldr_train):
                images, labels = images.to(self.args.device), labels.to(self.args.device)
                net.zero_grad()
                logits, log_probs = net(images)
                loss = F.cross_entropy(logits, labels)
                optimizer.zero_grad()
                # proximal_term
                proximal_term = 0.0
                for w, w_t in zip(net.parameters(), global_model.parameters()):
                    proximal_term += (w - w_t).norm(2)
                loss = loss + self.args.mu/2*proximal_term
                loss.backward()
                optimizer.step()

                batch_loss.append(loss.item())
            epoch_loss.append(sum(batch_loss)/len(batch_loss))

        return net.state_dict(), sum(epoch_loss) / len(epoch_loss), gen_loss

##########################################
#                  AvgKD                 #
##########################################

class Dataset_with_pseudo(Dataset):
    def __init__(self, images, labels, pseudo_labels, transform=False):
        super(Dataset_with_pseudo, self).__init__()
        self.images = images
        self.labels = labels
        self.pseudo_labels = pseudo_labels
        self.transform = transform
        
    def __len__(self):
        return self.images.size(0)

    def __getitem__(self, idx):
        image = self.images[idx]
        if torch.is_tensor(self.labels[idx]):
            label = self.labels[idx].item()
        else:
            label = self.labels[idx]
        
        pseudo_label = self.pseudo_labels[idx]
        
        return image, label, pseudo_label
    

class LocalUpdate_avgKD(object):
    def __init__(self, args, dataset=None, idxs=None):
        
        torch.manual_seed(args.rs)
        torch.cuda.manual_seed(args.rs)
        torch.cuda.manual_seed_all(args.rs) # if use multi-GPU
        np.random.seed(args.rs)
        
        self.args = args
        self.loss_func = nn.CrossEntropyLoss()
        self.selected_clients = []
        self.iter = len(idxs)//args.local_bs
        self.device = args.device
        self.ldr_train = DataLoader(DatasetSplit(dataset, idxs), batch_size=self.args.local_bs, shuffle=True)
    
        
    def train(self, local_nets, ws_glob, curr_idx, learning_rate, gennet=None):
        
        net = local_nets[curr_idx]
        net.load_state_dict(ws_glob[curr_idx])
        net = copy.deepcopy(net).to(self.device)
        net.train()
        
        optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=self.args.momentum, weight_decay=self.args.weight_decay)
        gen_epoch_loss = None
        gen_loss = None
        if gennet:
            gen_epoch_loss = []
            gennet.eval()
            # print('gen sample iteration',self.iter)
            for iter in range(self.args.local_ep_gen): # train by samples generated by generator
                gen_batch_loss = []
        
                for i in range(self.iter):
                    with torch.no_grad():
                        images, labels = gennet.sample_image(self.args, sample_num=self.args.local_bs) # images.shape (bs, feature^2)
                    net.zero_grad()
                    if self.args.models == 'mixnet':
                        logits = net(images)
                    else:
                        logits, log_probs = net(images)
                    
                    loss = F.cross_entropy(logits, labels) # net.fc1.weight.grad / net.fc5.weight.grad
                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()

                    gen_batch_loss.append(loss.item())
                gen_epoch_loss.append(sum(gen_batch_loss)/len(gen_batch_loss))     
            # gennet creates feature samples (gennet(, labels))
            if gen_epoch_loss:
                gen_loss = sum(gen_epoch_loss) / len(gen_epoch_loss)

        # train and update
        optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=self.args.momentum, weight_decay=self.args.weight_decay)
        epoch_loss = []

        for iter in range(self.args.local_ep): # train net performing main-task
            batch_loss = []

            if iter == 0:
                pseudo_labels = []
                images_total = []
                labels_total = []
                
                for batch_idx, (images, labels) in enumerate(self.ldr_train):
                    
                    images = images.to(self.args.device)
                    labels = labels.to(self.args.device)
                    pseudo_labels_per_batch = []
                    pseudo_labels_per_batch.append(F.one_hot(labels, num_classes=10)) # original label 추가 
                    
                    # logit from other models
                    with torch.no_grad(): 
                        for client_idx in range(len(local_nets)):
                            if client_idx != curr_idx: # pseudo label from each client model 
                                model = local_nets[client_idx] 
                                model.load_state_dict(ws_glob[client_idx])
                                client_net = copy.deepcopy(model).to(self.device)
                                client_net.eval()
                                if self.args.models == 'mixnet':
                                    diff_logits = client_net(images)   
                                else:
                                    diff_logits, log_probs = client_net(images)
                                softmax_logits = F.softmax(diff_logits, dim=1)
                                pseudo_labels_per_batch.append(softmax_logits)
                          
                    pseudo_label = F.softmax(sum(pseudo_labels_per_batch), dim=1)
                    
                    pseudo_labels.append(pseudo_label)  
                    images_total.append(images)
                    labels_total.append(labels)  

                images_total = torch.cat(images_total, 0).detach().cpu()
                labels_total = torch.cat(labels_total, 0).detach().cpu()
                pseudo_labels = torch.cat(pseudo_labels, 0).detach().cpu()

                new_dataset = Dataset_with_pseudo(images_total, labels_total, pseudo_labels)
                self.ldr_train = DataLoader(new_dataset, batch_size=self.args.local_bs, shuffle=True)        
            
            for batch_idx, (images, labels, pseudo_labels) in enumerate(self.ldr_train):
                    
                images, labels, pseudo_labels = images.to(self.args.device), labels.to(self.args.device), pseudo_labels.to(self.args.device)
                net.zero_grad()
                if self.args.models == 'mixnet':
                    logits = net(images)
                else:
                    logits, log_probs = net(images)
                
                loss = F.cross_entropy(logits, pseudo_labels)
                optimizer.zero_grad()
                loss.backward(retain_graph=True)
                optimizer.step()

                batch_loss.append(loss.item())
            epoch_loss.append(sum(batch_loss)/len(batch_loss))
              
        return net.state_dict(), sum(epoch_loss) / len(epoch_loss), gen_loss